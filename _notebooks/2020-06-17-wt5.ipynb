{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WT5?! Self-explanation of Language Models\n",
    "> Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. --- Richard Gall\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [language model, explainability]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "深度學習 (Deep Learning)，受惠於計算能力的成長以及計算框架的成熟，許多研究領域如: 視覺 (vision), 語音 (speech), 自然語言 (natural language) 在近5年有了飛躍性的技術成長。其中, 在自然語言處理 (Natural Language Processing) 領域, Delvin et. al. (2018) 提出的 BERT (Bidirectional Encoder Representations from Transformers), 在大量文本上的預訓練 (pre-train) 語言模型 (language model), 在眾多自然語言理解 (Natural Language Understanding) 任務達到了當時的最高技術水平 (state of the art, or SOTA)。 基於BERT的思想, 許多其改良版本如: [RoBERTa](), [SpanBERT](), [ALBERT](), [ELECTRA](), 致力於研究如何調整預訓練任務, 降低模型複雜度, 或是引入新的預訓練思想等等。有別於此研究方向, 2019年, [Colin et.al (2019)]() 與 [Lewis et. al (2019)]() 分別提出在預訓練階段重新導入了完整Transformer的Sequence-to-Sequence (Seq2Seq)架構, 而非BERT的單純編碼器 (Encoder), 或是GPT-2 (最新改良版本為GPT-3)的單純解碼器 (Decoder)結構。\n",
    "\n",
    "[Colin et. al (2019)]() 所提出的Text-To-Text Transfer Transformer (T5)模型不僅止於在pretrain階段使用Seq2Seq架構, 其Text-To-Text的思想將所有預訓練階段後的下游任務如: 閱讀理解 (reading comprehension), 句子相似度 (senetence similarity), 文字蘊含 (textual entailment)等等, 皆轉化為text in text out的形式。此方法連同其預訓練的模型, 在各大NLP排行榜刷新了最佳紀錄。 將模型所有任務的輸入與輸出皆轉化為text in text out為NLP領域帶來許多嶄新的應用方式與想像, 例如各NLP下游任務的多任務學習(Multi-task learning)可能帶來的模型推廣能力, 以及增進模型的可解釋性。\n",
    "\n",
    "BERT等基於深度神經網路 (Deep Neural Network)模型的在各大排行榜的標準測試平台上有著卓越的表現, 但要理解深度神經網路為何有效仍然是近年來的重大研究議題。 近年諸多方法皆基於神經網路之視覺化, 例如: (1) 將BERT中所使用的Self-attention架構的權重視覺化網路聯通圖, 並由研究者觀察其連接模式 (pattern) 賦予解釋。 (2) 將BERT學習到的向量表徵 (vector representations) 降維後, 由研究者觀察二維或三圍空間中向量點與點之間的關係做出推論。 有別於前述常使用於傳統機器學習的模型行為探測與解釋方法之外, T5所帶來的text-to-text架構使得一個新模式: 模型自行輸出文字, 研究員\n",
    "由輸出文字中觀察模式與設計測量方法, 成為了可能。\n",
    "\n",
    "本文介紹的是[Colin et. al (2020)]()的新工作, 作者使用T5架構, 將在NLP的解釋任務化為text-to-text的形式, 在解釋任務的訓練資料上進行微調 (finetuning) 後, 得到的新模型: WT5 (Why, T5?) 能夠以文字方式對其預測結果做出解釋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
