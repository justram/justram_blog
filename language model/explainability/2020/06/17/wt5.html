<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>WT5?! Self-explanation of Language Models | Matt J.H. Yang</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="WT5?! Self-explanation of Language Models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. — Richard Gall" />
<meta property="og:description" content="Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. — Richard Gall" />
<link rel="canonical" href="https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html" />
<meta property="og:url" content="https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html" />
<meta property="og:site_name" content="Matt J.H. Yang" />
<meta property="og:image" content="https://justram.github.io/justram_blog/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-17T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. — Richard Gall","@type":"BlogPosting","headline":"WT5?! Self-explanation of Language Models","dateModified":"2020-06-17T00:00:00-05:00","datePublished":"2020-06-17T00:00:00-05:00","image":"https://justram.github.io/justram_blog/images/chart-preview.png","url":"https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/justram_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://justram.github.io/justram_blog/feed.xml" title="Matt J.H. Yang" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-169628607-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/justram_blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>WT5?! Self-explanation of Language Models | Matt J.H. Yang</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="WT5?! Self-explanation of Language Models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. — Richard Gall" />
<meta property="og:description" content="Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. — Richard Gall" />
<link rel="canonical" href="https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html" />
<meta property="og:url" content="https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html" />
<meta property="og:site_name" content="Matt J.H. Yang" />
<meta property="og:image" content="https://justram.github.io/justram_blog/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-17T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. — Richard Gall","@type":"BlogPosting","headline":"WT5?! Self-explanation of Language Models","dateModified":"2020-06-17T00:00:00-05:00","datePublished":"2020-06-17T00:00:00-05:00","image":"https://justram.github.io/justram_blog/images/chart-preview.png","url":"https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://justram.github.io/justram_blog/feed.xml" title="Matt J.H. Yang" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-169628607-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/justram_blog/">Matt J.H. Yang</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/justram_blog/about/">About Me</a><a class="page-link" href="/justram_blog/search/">Search</a><a class="page-link" href="/justram_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">WT5?! Self-explanation of Language Models</h1><p class="page-description">Explainability, meanwhile, is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. --- Richard Gall</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-17T00:00:00-05:00" itemprop="datePublished">
        Jun 17, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/justram_blog/categories/#language model">language model</a>
        &nbsp;
      
        <a class="category-tags-link" href="/justram_blog/categories/#explainability">explainability</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/justram/justram_blog/tree/master/_notebooks/2020-06-17-wt5.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/justram_blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/justram/justram_blog/master?filepath=_notebooks%2F2020-06-17-wt5.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/justram_blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/justram/justram_blog/blob/master/_notebooks/2020-06-17-wt5.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/justram_blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Intro">Intro </a></li>
<li class="toc-entry toc-h1"><a href="#Approach">Approach </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-17-wt5.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Intro">
<a class="anchor" href="#Intro" aria-hidden="true"><span class="octicon octicon-link"></span></a>Intro<a class="anchor-link" href="#Intro"> </a>
</h1>
<p>深度學習 (Deep Learning)，受惠於計算能力的成長以及計算框架的成熟，許多研究領域如: 視覺 (vision), 語音 (speech), 自然語言 (natural language) 在近5年有了飛躍性的技術成長。其中, 在自然語言處理 (Natural Language Processing) 領域, Delvin et. al. (2018) 提出的 BERT (Bidirectional Encoder Representations from Transformers), 在大量文本上的預訓練 (pre-train) 語言模型 (language model), 在眾多自然語言理解 (Natural Language Understanding) 任務達到了當時的最高技術水平 (state of the art, or SOTA)。 基於BERT的思想, 許多其改良版本如: <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>, <a href="https://arxiv.org/abs/1907.10529">SpanBERT</a>, <a href="https://arxiv.org/abs/1909.11942">ALBERT</a>, <a href="https://openreview.net/pdf?id=r1xMH1BtvB">ELECTRA</a>, 致力於研究如何調整預訓練任務, 降低模型複雜度, 或是引入新的預訓練思想等等。有別於此研究方向, 2019年, <a href="https://arxiv.org/abs/1910.10683">Colin et.al (2019)</a> 與 <a href="https://arxiv.org/abs/1910.13461">Lewis et. al (2019)</a> 分別提出在預訓練階段重新導入了完整Transformer的Sequence-to-Sequence (Seq2Seq)架構, 而非BERT的單純編碼器 (Encoder), 或是<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">GPT-2</a> (最新改良版本為<a href="https://arxiv.org/abs/2005.14165">GPT-3</a>)的單純解碼器 (Decoder)結構。</p>
<p><a href="https://arxiv.org/abs/1910.10683">Colin et. al (2019)</a> 所提出的Text-To-Text Transfer Transformer (T5)模型不僅止於在pretrain階段使用Seq2Seq架構, 其Text-To-Text的思想將所有預訓練階段後的下游任務如: 閱讀理解 (reading comprehension), 句子相似度 (senetence similarity), 文字蘊含 (textual entailment)等等, 皆轉化為text in text out的形式。此方法連同其預訓練的模型, 在各大NLP排行榜刷新了最佳紀錄。 將模型所有任務的輸入與輸出皆轉化為text in text out為NLP領域帶來許多嶄新的應用方式與想像, 例如各NLP下游任務的多任務學習(Multi-task learning)可能帶來的模型推廣能力, 以及增進模型的可解釋性。</p>
<p>BERT等基於深度神經網路 (Deep Neural Network)模型的在各大排行榜的標準測試平台上有著卓越的表現, 但要理解深度神經網路為何有效仍然是近年來的重大研究議題。 近年諸多方法皆基於神經網路之視覺化, 例如: (1) 將BERT中所使用的Self-attention架構的權重視覺化網路聯通圖, 並由研究者觀察其連接模式 (pattern) 賦予解釋。 (2) 將BERT學習到的向量表徵 (vector representations) 降維後, 由研究者觀察二維或三維空間中向量點與點之間的關係做出推論。 有別於前述常使用於傳統機器學習的模型行為探測與解釋方法之外, T5所帶來的text-to-text架構使得一個新模式: 模型自行輸出文字, 研究員
由輸出文字中觀察模式與設計測量方法, 成為了可能。</p>
<p>本文介紹的是<a href="https://arxiv.org/abs/2004.14546">Colin et. al (2020)</a>的新工作, 作者使用T5架構, 將在NLP的解釋任務化為text-to-text的形式, 在解釋任務的訓練資料上進行微調 (finetuning) 後, 得到的新模型: WT5 (Why, T5?) 能夠以文字方式對其預測結果做出解釋。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Approach">
<a class="anchor" href="#Approach" aria-hidden="true"><span class="octicon octicon-link"></span></a>Approach<a class="anchor-link" href="#Approach"> </a>
</h1>
<p>接下來, 我們藉由<a href="https://github.com/huggingface/transformers">Transformers</a> --- developed by <a href="https://huggingface.co/">Hugging Face</a> --- 這個開源模型庫來讓大家熟悉Intro中所提到的概念。</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install transformers
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting transformers
  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)
     |████████████████████████████████| 675kB 2.8MB/s eta 0:00:01
Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)
Requirement already satisfied: dataclasses; python_version &lt; "3.7" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)
Collecting sentencepiece
  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)
     |████████████████████████████████| 1.1MB 14.5MB/s 
Collecting sacremoses
  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)
     |████████████████████████████████| 890kB 18.6MB/s 
Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)
Collecting tokenizers==0.7.0
  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)
     |████████████████████████████████| 3.8MB 24.0MB/s 
Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)
Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2020.4.5.2)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (1.24.3)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2.9)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (3.0.4)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (1.12.0)
Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (7.1.2)
Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (0.15.1)
Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers) (2.4.7)
Building wheels for collected packages: sacremoses
  Building wheel for sacremoses (setup.py) ... done
  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4411973ff5b191e8f18328dac9811d4d5cc4be6ae2407d238f34dc1b9c7aa6c3
  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45
Successfully built sacremoses
Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers
Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>首先, 引入必要的物件:
1) 模型: T5ForConditionalGeneration,
2) 純文字至模型輸入轉換: T5Tokenizer</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">T5ForConditionalGeneration</span><span class="p">,</span> <span class="n">T5Tokenizer</span>

<span class="c1"># Pretrained model name for a specific architecture and tokenization</span>
<span class="n">pretrained_weights</span> <span class="o">=</span> <span class="s1">'t5-small'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">T5Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">T5ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>


</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>為了沒有使用NLP模型基礎的讀者, 此處我們演示一下從文字輸入如何轉換到模型真正看到的輸入。 這裡讀者們需要理解的觀念為: 一般而言, 我們所使用的純文字必須經過將字符轉換為該字符所對應的代號, NLP模型才能才能進行進一步的處理。</p>
<p>如下所示, 我們將"Here is some text to encode"這一連串的字串 (string) 資料, 將字符組 (character set) 經過tokenizer轉換為對應的字符組對應編號 (id) 。</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_text</span> <span class="o">=</span> <span class="s2">"Here is some text to encode"</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>
<span class="n">sep</span> <span class="o">=</span> <span class="s2">"="</span><span class="o">*</span><span class="mi">10</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"original text:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">test_text</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">sep</span><span class="si">}</span><span class="se">\n</span><span class="s2">tokenized id:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">input_ids</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>original text:
 Here is some text to encode
==========
tokenized id:
 tensor([[  947,    19,   128,  1499,    12, 23734]])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>這裏我們測試一下上述的input ids經過T5模型之後, 會產生什麼樣的輸出呢?
因為T5是一個Seq2Seq模型, 給定輸入之後, 模型可以根據給定的輸入而產生輸出。</p>
<p>Note: 模型的輸出不做處理的話也會是id, 須經過tokenizer再做一次轉換方可轉換為文字, 達成前面我們所說的text-to-text架構。</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">output_detok</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_detok</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>['encode Here is some text to encode Here is some text to encode Here is']
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>可以看到, 如果輸入的文字沒有特別的pattern, 經過預訓練的T5不會產出特殊的輸出文字。 然而, 若是T5曾經在NLP下游任務上做過text-to-text的finetuning, 給定對應的文字pattern, T5是會直接吐出我們想要的文字輸出的!</p>
<p>下面我們透過<a href="">QNLI</a>這個T5曾經被訓練過的NLP下游任務, 給定對應的文字pattern, 看看T5是不是會輸出有趣的東西。</p>
<p>Question Natural Language Inference (QNLI), 這是一個從經典閱讀理解資料集 --- SQUAD v1.0 --- 中衍伸出來的資料集, 在這個資料集中, 我們期望模型能夠學會判斷, 對於某question而言, 該sentence中是否含有答案? 此為一個二分類任務, 輸出為entailment (label=0) 或 not_entailment (label=1)。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>首先我們給定一個template"qnli question:  sentence: ", 藉由tokenizer轉換後給T5輸出, 我們會發現T5知道這個句子中含有答案!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"Where did Jebe die?"</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">"Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand."</span>
<span class="n">qnli_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"qnli question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2"> sentence: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="s2">"</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">qnli_text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">output_detok</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_detok</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>['entailment']
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>若我們做個實驗, 將輸入句子只留下前十個characters, T5會告訴我們什麼呢?</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">qnli_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"qnli question: </span><span class="si">{</span><span class="n">question</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2"> sentence: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="s2">"</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">qnli_text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">output_detok</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_detok</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>['not_entailment']
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Amazing! T5看起來是對於這樣的QNLI問題文字pattern產生反應的, 也能在一定程度上回答出正確的答案。</p>
<p>經過上面的演示, 相信讀者已經了解T5經過特定的文字pattern訓練後, 是能夠對於特定的pattern給出其受過訓練的預測的, 接下來我們將進入WT5論文內容的解讀</p>
<p>試想, 若是我們能夠給定訓練文字pattern如:</p>
<p>輸入:"<strong><em>explain</em></strong> qnli question:  sentence: "&lt;/p&gt;
</p>
<p>輸出:""&lt;/p&gt;
</p>
<p>是否我們就能讓T5除了原本給出的entailment/not entailment分類答案以外, 還能看到explain就輸出它為何會做這樣的分類判斷呢?</p>

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
 

<script type="application/vnd.jupyter.widget-state+json">
{"0d55595312af47b58ac7e737988f1252": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_8a750d81d93947a8ab72c20b30aa933b", "placeholder": "\u200b", "style": "IPY_MODEL_a307a48c0ae246fc89737d2a3987f0eb", "value": " 792k/792k [00:00&lt;00:00, 842kB/s]"}}, "0ec6f5d035854ae3aa2e5c165b1608a6": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "19b43fa6f4474f32ac7fa91df4e846dd": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "30652817e6fd4a96bd65bf2ab348009e": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3369aa3c5216446aa8a6837c378c1d6c": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "355d390ffd944514a1885ad65c3b610e": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "35ee7e2667a14e38b67ffeeb582e1f04": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3f4a2457d4bd4d2ea282c2cde7fcea97": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "42cd1cbbe5304144bb842adc86e25e92": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "45a2033bb2b64247a68aaa1f066c728b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "47118c62b3ef4cd4bfef954ba8a275b1": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_cf829c1151fd41d0ba60a19f9f50e122", "IPY_MODEL_66c2aec2732045798819d240d153bc23"], "layout": "IPY_MODEL_f6c1b291842f422d98875ab3049ca45b"}}, "66c2aec2732045798819d240d153bc23": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_35ee7e2667a14e38b67ffeeb582e1f04", "placeholder": "\u200b", "style": "IPY_MODEL_19b43fa6f4474f32ac7fa91df4e846dd", "value": " 242M/242M [00:05&lt;00:00, 47.4MB/s]"}}, "81732920262b417da25bd046c53372c3": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8a750d81d93947a8ab72c20b30aa933b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8c212a44e04145ed9badace4822ab07c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "90490c4924cc42ec847c3b2674f00ab9": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_d53fe158e4fc45ddb4024c965b7b67ce", "IPY_MODEL_ecbf5ab2ccfd42c6871594602617ee11"], "layout": "IPY_MODEL_0ec6f5d035854ae3aa2e5c165b1608a6"}}, "97e4559267194be89b0a1b65b4eca32d": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_d3512005069744a5bb69759e64933c06", "IPY_MODEL_0d55595312af47b58ac7e737988f1252"], "layout": "IPY_MODEL_30652817e6fd4a96bd65bf2ab348009e"}}, "99fea38f75d046838d36d40378b3f639": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "a307a48c0ae246fc89737d2a3987f0eb": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "cf829c1151fd41d0ba60a19f9f50e122": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: 100%", "description_tooltip": null, "layout": "IPY_MODEL_45a2033bb2b64247a68aaa1f066c728b", "max": 242136741, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_99fea38f75d046838d36d40378b3f639", "value": 242136741}}, "d3512005069744a5bb69759e64933c06": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: 100%", "description_tooltip": null, "layout": "IPY_MODEL_8c212a44e04145ed9badace4822ab07c", "max": 791656, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_3f4a2457d4bd4d2ea282c2cde7fcea97", "value": 791656}}, "d53fe158e4fc45ddb4024c965b7b67ce": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: 100%", "description_tooltip": null, "layout": "IPY_MODEL_81732920262b417da25bd046c53372c3", "max": 1197, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_42cd1cbbe5304144bb842adc86e25e92", "value": 1197}}, "ecbf5ab2ccfd42c6871594602617ee11": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_355d390ffd944514a1885ad65c3b610e", "placeholder": "\u200b", "style": "IPY_MODEL_3369aa3c5216446aa8a6837c378c1d6c", "value": " 1.20k/1.20k [00:00&lt;00:00, 3.26kB/s]"}}, "f6c1b291842f422d98875ab3049ca45b": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}}
</script>

根據該問題準備的解釋文字&gt;欲判斷之句子&gt;欲給定的問題&gt;</div>
</div></div>欲判斷之句子&gt;欲給定的問題&gt;</div>
</div></div>
</div>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="justram/justram_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/justram_blog/language%20model/explainability/2020/06/17/wt5.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/justram_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/justram_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/justram_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A personal blog about machine leaning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/justram" title="justram"><svg class="svg-icon grey"><use xlink:href="/justram_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/mattyan76932122" title="mattyan76932122"><svg class="svg-icon grey"><use xlink:href="/justram_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
