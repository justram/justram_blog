{
  
    
        "post0": {
            "title": "WT5?! Self-explanation of Language Models",
            "content": "Intro . 深度學習 (Deep Learning)，受惠於計算能力的成長以及計算框架的成熟，許多研究領域如: 視覺 (vision), 語音 (speech), 自然語言 (natural language) 在近5年有了飛躍性的技術成長。其中, 在自然語言處理 (Natural Language Processing) 領域, Delvin et. al. (2018) 提出的 BERT (Bidirectional Encoder Representations from Transformers), 在大量文本上的預訓練 (pre-train) 語言模型 (language model), 在眾多自然語言理解 (Natural Language Understanding) 任務達到了當時的最高技術水平 (state of the art, or SOTA)。 基於BERT的思想, 許多其改良版本如: RoBERTa, SpanBERT, ALBERT, ELECTRA, 致力於研究如何調整預訓練任務, 降低模型複雜度, 或是引入新的預訓練思想等等。有別於此研究方向, 2019年, Colin et.al (2019) 與 Lewis et. al (2019) 分別提出在預訓練階段重新導入了完整Transformer的Sequence-to-Sequence (Seq2Seq)架構, 而非BERT的單純編碼器 (Encoder), 或是GPT-2 (最新改良版本為GPT-3)的單純解碼器 (Decoder)結構。 . Colin et. al (2019) 所提出的Text-To-Text Transfer Transformer (T5)模型不僅止於在pretrain階段使用Seq2Seq架構, 其Text-To-Text的思想將所有預訓練階段後的下游任務如: 閱讀理解 (reading comprehension), 句子相似度 (senetence similarity), 文字蘊含 (textual entailment)等等, 皆轉化為text in text out的形式。此方法連同其預訓練的模型, 在各大NLP排行榜刷新了最佳紀錄。 將模型所有任務的輸入與輸出皆轉化為text in text out為NLP領域帶來許多嶄新的應用方式與想像, 例如各NLP下游任務的多任務學習(Multi-task learning)可能帶來的模型推廣能力, 以及增進模型的可解釋性。 . BERT等基於深度神經網路 (Deep Neural Network)模型的在各大排行榜的標準測試平台上有著卓越的表現, 但要理解深度神經網路為何有效仍然是近年來的重大研究議題。 近年諸多方法皆基於神經網路之視覺化, 例如: (1) 將BERT中所使用的Self-attention架構的權重視覺化網路聯通圖, 並由研究者觀察其連接模式 (pattern) 賦予解釋。 (2) 將BERT學習到的向量表徵 (vector representations) 降維後, 由研究者觀察二維或三維空間中向量點與點之間的關係做出推論。 有別於前述常使用於傳統機器學習的模型行為探測與解釋方法之外, T5所帶來的text-to-text架構使得一個新模式: 模型自行輸出文字, 研究員 由輸出文字中觀察模式與設計測量方法, 成為了可能。 . 本文介紹的是Colin et. al (2020)的新工作, 作者使用T5架構, 將在NLP的解釋任務化為text-to-text的形式, 在解釋任務的訓練資料上進行微調 (finetuning) 後, 得到的新模型: WT5 (Why, T5?) 能夠以文字方式對其預測結果做出解釋。 . Approach . 接下來, 我們藉由Transformers developed by Hugging Face 這個開源模型庫來讓大家熟悉Intro中所提到的概念。 . !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB) |████████████████████████████████| 675kB 2.8MB/s eta 0:00:01 Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Collecting sentencepiece Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB) |████████████████████████████████| 1.1MB 14.5MB/s Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |████████████████████████████████| 890kB 18.6MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Collecting tokenizers==0.7.0 Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB) |████████████████████████████████| 3.8MB 24.0MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2020.4.5.2) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2.9) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (1.12.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (0.15.1) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers) (2.4.7) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4411973ff5b191e8f18328dac9811d4d5cc4be6ae2407d238f34dc1b9c7aa6c3 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0 . 首先, 引入必要的物件: 1) 模型: T5ForConditionalGeneration, 2) 純文字至模型輸入轉換: T5Tokenizer . import torch from transformers import T5ForConditionalGeneration, T5Tokenizer # Pretrained model name for a specific architecture and tokenization pretrained_weights = &#39;t5-small&#39; tokenizer = T5Tokenizer.from_pretrained(pretrained_weights) model = T5ForConditionalGeneration.from_pretrained(pretrained_weights) . . 為了沒有使用NLP模型基礎的讀者, 此處我們演示一下從文字輸入如何轉換到模型真正看到的輸入。 這裡讀者們需要理解的觀念為: 一般而言, 我們所使用的純文字必須經過將字符轉換為該字符所對應的代號, NLP模型才能才能進行進一步的處理。 . 如下所示, 我們將&quot;Here is some text to encode&quot;這一連串的字串 (string) 資料, 將字符組 (character set) 經過tokenizer轉換為對應的字符組對應編號 (id) 。 . test_text = &quot;Here is some text to encode&quot; input_ids = torch.tensor([tokenizer.encode(test_text, add_special_tokens=True)]) sep = &quot;=&quot;*10 print(f&quot;original text: n {test_text} n{sep} ntokenized id: n {input_ids}&quot;) . original text: Here is some text to encode ========== tokenized id: tensor([[ 947, 19, 128, 1499, 12, 23734]]) . 這裏我們測試一下上述的input ids經過T5模型之後, 會產生什麼樣的輸出呢? 因為T5是一個Seq2Seq模型, 給定輸入之後, 模型可以根據給定的輸入而產生輸出。 . Note: 模型的輸出不做處理的話也會是id, 須經過tokenizer再做一次轉換方可轉換為文字, 達成前面我們所說的text-to-text架構。 . output = model.generate(input_ids=input_ids, max_length=16) output_detok = [tokenizer.decode(token) for token in output] print(output_detok) . [&#39;encode Here is some text to encode Here is some text to encode Here is&#39;] . 可以看到, 如果輸入的文字沒有特別的pattern, 經過預訓練的T5不會產出特殊的輸出文字。 然而, 若是T5曾經在NLP下游任務上做過text-to-text的finetuning, 給定對應的文字pattern, T5是會直接吐出我們想要的文字輸出的! . 下面我們透過QNLI這個T5曾經被訓練過的NLP下游任務, 給定對應的文字pattern, 看看T5是不是會輸出有趣的東西。 . Question Natural Language Inference (QNLI), 這是一個從經典閱讀理解資料集 SQUAD v1.0 中衍伸出來的資料集, 在這個資料集中, 我們期望模型能夠學會判斷, 對於某question而言, 該sentence中是否含有答案? 此為一個二分類任務, 輸出為entailment (label=0) 或 not_entailment (label=1)。 . 首先我們給定一個template&quot;qnli question: {欲給定的問題} sentence: {欲判斷之句子}&quot;, 藉由tokenizer轉換後給T5輸出, 我們會發現T5知道這個句子中含有答案! . question = &quot;Where did Jebe die?&quot; sentence = &quot;Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand.&quot; qnli_text = f&quot;qnli question: {question} sentence: {sentence}&quot; input_ids = torch.tensor([tokenizer.encode(qnli_text, add_special_tokens=True)]) output = model.generate(input_ids=input_ids, max_length=16) output_detok = [tokenizer.decode(token) for token in output] print(output_detok) . [&#39;entailment&#39;] . 若我們做個實驗, 將輸入句子只留下前十個characters, T5會告訴我們什麼呢? . qnli_text = f&quot;qnli question: {question[:10]} sentence: {sentence}&quot; input_ids = torch.tensor([tokenizer.encode(qnli_text, add_special_tokens=True)]) output = model.generate(input_ids=input_ids, max_length=16) output_detok = [tokenizer.decode(token) for token in output] print(output_detok) . [&#39;not_entailment&#39;] . Amazing! T5看起來是對於這樣的QNLI問題文字pattern產生反應的, 也能在一定程度上回答出正確的答案。 . 經過上面的演示, 相信讀者已經了解T5經過特定的文字pattern訓練後, 是能夠對於特定的pattern給出其受過訓練的預測的, 接下來我們將進入WT5論文內容的解讀 . 試想, 若是我們能夠給定訓練文字pattern如: . 輸入:&quot;explain qnli question: {欲給定的問題} sentence: {欲判斷之句子}&quot; . 輸出:&quot;{根據該問題準備的解釋文字}&quot; . 是否我們就能讓T5除了原本給出的entailment/not entailment分類答案以外, 還能看到explain就輸出它為何會做這樣的分類判斷呢? .",
            "url": "https://justram.github.io/justram_blog/language%20model/explainability/2020/06/17/wt5.html",
            "relUrl": "/language%20model/explainability/2020/06/17/wt5.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://justram.github.io/justram_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://justram.github.io/justram_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://justram.github.io/justram_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://justram.github.io/justram_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}